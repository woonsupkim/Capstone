{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1330f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5516cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aggregated_handlabeled.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5238e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN in label columns with 0\n",
    "columns = ['cost','ease of use', 'effective', 'efficient']\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].replace(np.nan, int(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d47d09",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58727501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c91f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "sentiment = [None] * len(df)\n",
    "index = -1\n",
    "for sentence in df['review_lower'][0:99]:\n",
    "    index+=1\n",
    "    if(index%20 == 0):\n",
    "        print(index)\n",
    "\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "df['sentiment_m'] = sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6cf8c",
   "metadata": {},
   "source": [
    "# Zero-shot classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2dce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = pipeline(\"zero-shot-classification\",  model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763a9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the labels you identified from above section\n",
    "type = ['cost', 'efficient', 'effective', 'ease of use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5cdc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#setting empty values for the columns\n",
    "index = -1\n",
    "for label in type:\n",
    "    df[label + '_m'] = -1\n",
    "\n",
    "for j in range(99):\n",
    "    #counter for progress/debugging\n",
    "    index+=1\n",
    "    if(index%20 == 0): \n",
    "        print(index)\n",
    "        \n",
    "    #running the classifier on the column    \n",
    "    res = classifier(\n",
    "        df.iloc[j]['review_lower'],\n",
    "        candidate_labels = type,\n",
    "        multi_label = True\n",
    "    )\n",
    "    #setting the column values according to the output from the classifier (\"_m\" = multiclass)\n",
    "    for i in range(len(res['labels'])):\n",
    "        df[res['labels'][i]+ '_m'].iloc[j] = res['scores'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34bcb7",
   "metadata": {},
   "source": [
    "# Sentiment Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0cdc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation1(list):\n",
    "    i = []\n",
    "    for prediction in list:\n",
    "        if prediction == \"Positive\":\n",
    "            i.append(1)\n",
    "        elif prediction == \"Neutral\":\n",
    "            i.append(0)\n",
    "        else:\n",
    "            i.append(-1)    \n",
    "    return(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a9afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = imputation1(df['sentiment'])\n",
    "df['sentiment_m'] = imputation1(df['sentiment_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fdb9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5c1301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.82      0.90       400\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.33      0.27      0.30       400\n",
      "weighted avg       1.00      0.82      0.90       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['sentiment'], df['sentiment_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "346d01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_sentiment = classification_report(df['sentiment'], df['sentiment_m'], output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e285ab0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall meaning: Out of all the sentiments that actually did get drafted, the model only predicted this outcome correctly for 82.25% of those.\n",
    "report_sentiment['accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
