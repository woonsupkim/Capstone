{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Wonjoo/PenningtonFertilizer.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN in label columns with 0\n",
    "columns = ['cost','ease of use', 'effective', 'efficient']\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].replace(np.nan, int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = pipeline(\"zero-shot-classification\",  model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the labels you identified from above section\n",
    "type = ['cost', 'efficient', 'effective', 'ease of use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#setting empty values for the columns\n",
    "index = -1\n",
    "for label in type:\n",
    "    df[label + '_m'] = -1\n",
    "\n",
    "for j in range(99):\n",
    "    #counter for progress/debugging\n",
    "    index+=1\n",
    "    if(index%20 == 0): \n",
    "        print(index)\n",
    "        \n",
    "    #running the classifier on the column    \n",
    "    res = classifier(\n",
    "        df.iloc[j]['review_lower'],\n",
    "        candidate_labels = type,\n",
    "        multi_label = True\n",
    "    )\n",
    "    #setting the column values according to the output from the classifier (\"_m\" = multiclass)\n",
    "    for i in range(len(res['labels'])):\n",
    "        df[res['labels'][i]+ '_m'].iloc[j] = res['scores'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(list):\n",
    "    i = []\n",
    "    for prediction in list:\n",
    "        if prediction < 0.7:\n",
    "            i.append(0)\n",
    "        else:\n",
    "            i.append(1)\n",
    "    \n",
    "    return(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost_m'] = imputation(df['cost_m'])\n",
    "df['efficient_m'] = imputation(df['efficient_m'])\n",
    "df['effective_m'] = imputation(df['effective_m'])\n",
    "df['ease of use_m'] = imputation(df['ease of use_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99       395\n",
      "         1.0       0.31      1.00      0.48         5\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.66      0.99      0.73       400\n",
      "weighted avg       0.99      0.97      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['cost'], df['cost_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.9861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ROC_AUC = roc_auc_score(df['cost'], df['cost_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92       375\n",
      "         1.0       0.28      0.80      0.41        25\n",
      "\n",
      "    accuracy                           0.86       400\n",
      "   macro avg       0.63      0.83      0.67       400\n",
      "weighted avg       0.94      0.86      0.89       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['efficient'], df['efficient_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.8307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ROC_AUC = roc_auc_score(df['efficient'], df['efficient_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       316\n",
      "         1.0       0.85      0.83      0.84        84\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.90      0.90      0.90       400\n",
      "weighted avg       0.93      0.94      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['effective'], df['effective_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.8977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ROC_AUC = roc_auc_score(df['effective'], df['effective_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96       376\n",
      "         1.0       0.41      0.58      0.48        24\n",
      "\n",
      "    accuracy                           0.93       400\n",
      "   macro avg       0.69      0.77      0.72       400\n",
      "weighted avg       0.94      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['ease of use'], df['ease of use_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.7651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ROC_AUC = roc_auc_score(df['ease of use'], df['ease of use_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('testing_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv('testing_evaluation.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       395\n",
      "           1       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.49      0.50      0.50       400\n",
      "weighted avg       0.98      0.99      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation(df2['cost'], df2['naive_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# ROC_AUC = roc_auc_score(df2['cost'], df2['naive_cost'])\n",
    "# print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       375\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.88      0.94      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation(df2['efficient'], df2['naive_efficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# ROC_AUC = roc_auc_score(df2['efficient'], df2['naive_efficient'])\n",
    "# print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       316\n",
      "           1       0.21      1.00      0.35        84\n",
      "\n",
      "    accuracy                           0.21       400\n",
      "   macro avg       0.10      0.50      0.17       400\n",
      "weighted avg       0.04      0.21      0.07       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation(df2['effective'], df2['naive_effective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# ROC_AUC = roc_auc_score(df2['effective'], df2['naive_effective'])\n",
    "# print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       376\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.88      0.94      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation(df2['ease of use'], df2['naive_easeofuse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.5000\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# ROC_AUC = roc_auc_score(df2['ease of use'], df2['naive_easeofuse'])\n",
    "# print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "998c911629ba937bcf1bf80465453e12e8c5c2c818cb936f93ef7cf495a937a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
