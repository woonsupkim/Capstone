{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Wonjoo/PenningtonFertilizer.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN in label columns with 0\n",
    "columns = ['cost','ease of use', 'effective', 'efficient']\n",
    "\n",
    "for column in columns:\n",
    "    df[column] = df[column].replace(np.nan, int(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "sentiment = [None] * len(df)\n",
    "index = -1\n",
    "for sentence in df['review_lower'][0:99]:\n",
    "    index+=1\n",
    "    if(index%20 == 0):\n",
    "        print(index)\n",
    "\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "df['sentiment_m'] = sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = pipeline(\"zero-shot-classification\",  model = \"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the labels you identified from above section\n",
    "type = ['cost', 'efficient', 'effective', 'ease of use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#setting empty values for the columns\n",
    "index = -1\n",
    "for label in type:\n",
    "    df[label + '_m'] = -1\n",
    "\n",
    "for j in range(99):\n",
    "    #counter for progress/debugging\n",
    "    index+=1\n",
    "    if(index%20 == 0): \n",
    "        print(index)\n",
    "        \n",
    "    #running the classifier on the column    \n",
    "    res = classifier(\n",
    "        df.iloc[j]['review_lower'],\n",
    "        candidate_labels = type,\n",
    "        multi_label = True\n",
    "    )\n",
    "    #setting the column values according to the output from the classifier (\"_m\" = multiclass)\n",
    "    for i in range(len(res['labels'])):\n",
    "        df[res['labels'][i]+ '_m'].iloc[j] = res['scores'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation1(list):\n",
    "    i = []\n",
    "    for prediction in list:\n",
    "        if prediction == \"Positive\":\n",
    "            i.append(1)\n",
    "        elif prediction == \"Neutral\":\n",
    "            i.append(0)\n",
    "        else:\n",
    "            i.append(-1)    \n",
    "    return(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = imputation1(df['sentiment'])\n",
    "df['sentiment_m'] = imputation1(df['sentiment_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.79      0.88       400\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.33      0.26      0.29       400\n",
      "weighted avg       1.00      0.79      0.88       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['sentiment'], df['sentiment_m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(list):\n",
    "    i = []\n",
    "    for prediction in list:\n",
    "        if prediction < 0.7:\n",
    "            i.append(0)\n",
    "        else:\n",
    "            i.append(1)\n",
    "    \n",
    "    return(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cost_m'] = imputation(df['cost_m'])\n",
    "df['efficient_m'] = imputation(df['efficient_m'])\n",
    "df['effective_m'] = imputation(df['effective_m'])\n",
    "df['ease of use_m'] = imputation(df['ease of use_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99       395\n",
      "         1.0       0.31      1.00      0.48         5\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.66      0.99      0.73       400\n",
      "weighted avg       0.99      0.97      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['cost'], df['cost_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.9861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ROC_AUC = roc_auc_score(df['cost'], df['cost_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.86      0.92       375\n",
      "         1.0       0.28      0.80      0.41        25\n",
      "\n",
      "    accuracy                           0.86       400\n",
      "   macro avg       0.63      0.83      0.67       400\n",
      "weighted avg       0.94      0.86      0.89       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['efficient'], df['efficient_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.8307\n"
     ]
    }
   ],
   "source": [
    "ROC_AUC = roc_auc_score(df['efficient'], df['efficient_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       316\n",
      "         1.0       0.85      0.83      0.84        84\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.90      0.90      0.90       400\n",
      "weighted avg       0.93      0.94      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['effective'], df['effective_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.8977\n"
     ]
    }
   ],
   "source": [
    "ROC_AUC = roc_auc_score(df['effective'], df['effective_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96       376\n",
      "         1.0       0.41      0.58      0.48        24\n",
      "\n",
      "    accuracy                           0.93       400\n",
      "   macro avg       0.69      0.77      0.72       400\n",
      "weighted avg       0.94      0.93      0.93       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['ease of use'], df['ease of use_m']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC : 0.7651\n"
     ]
    }
   ],
   "source": [
    "ROC_AUC = roc_auc_score(df['ease of use'], df['ease of use_m'])\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Zero-Shot against a Naive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive(list):\n",
    "    avg  = np.mean(list)\n",
    "\n",
    "    if avg >= 0.5:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = 0\n",
    "\n",
    "    return([i] * len(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['naive_cost'] = naive(df['cost'])\n",
    "df['naive_efficient'] = naive(df['efficient'])\n",
    "df['naive_effective'] = naive(df['effective'])\n",
    "df['naive_easeofuse'] = naive(df['ease of use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       395\n",
      "         1.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.49      0.50      0.50       400\n",
      "weighted avg       0.98      0.99      0.98       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['cost'], df['naive_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97       375\n",
      "         1.0       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.88      0.94      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['efficient'], df['naive_efficient']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      1.00      0.88       316\n",
      "         1.0       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.40      0.50      0.44       400\n",
      "weighted avg       0.62      0.79      0.70       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['effective'], df['naive_effective']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97       376\n",
      "         1.0       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.47      0.50      0.48       400\n",
      "weighted avg       0.88      0.94      0.91       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['ease of use'], df['naive_easeofuse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('evaluation_PenningtonFertilizer.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "998c911629ba937bcf1bf80465453e12e8c5c2c818cb936f93ef7cf495a937a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
